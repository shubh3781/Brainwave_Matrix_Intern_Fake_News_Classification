{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a478e49e",
   "metadata": {},
   "source": [
    "# Fake News Detection using Machine Learning on the WELFake Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In today's digital age, the proliferation of fake news has become a significant concern. The ability to automatically detect and classify news articles as real or fake is crucial for maintaining the integrity of information. This project aims to develop a machine learning model that can accurately classify news articles using the WELFake dataset. This notebook documents the entire process, from data loading and preprocessing to model training, evaluation, and deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Importing Libraries](#importing-libraries)\n",
    "2. [Loading and Exploring the Data](#loading-and-exploring-the-data)\n",
    "   - [Dataset Description](#dataset-description)\n",
    "   - [Initial Data Inspection](#initial-data-inspection)\n",
    "3. [Data Preprocessing](#data-preprocessing)\n",
    "   - [Handling Missing Values](#handling-missing-values)\n",
    "   - [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "     - [Text Length Analysis](#text-length-analysis)\n",
    "   - [Text Cleaning](#text-cleaning)\n",
    "     - [Tokenization, Stopword Removal, and Stemming](#text-cleaning-steps)\n",
    "4. [Feature Extraction](#feature-extraction)\n",
    "   - [TF-IDF Vectorization](#tf-idf-vectorization)\n",
    "5. [Model Training](#model-training)\n",
    "   - [Train-Test Split](#train-test-split)\n",
    "   - [Multinomial Naive Bayes Classifier](#multinomial-naive-bayes-classifier)\n",
    "   - [Random Forest Classifier](#random-forest-classifier)\n",
    "6. [Model Evaluation](#model-evaluation)\n",
    "   - [Classification Reports](#classification-reports)\n",
    "7. [Model Saving and Deployment](#model-saving-and-deployment)\n",
    "   - [Saving Models with Pickle](#saving-models-with-pickle)\n",
    "   - [Loading Models for Prediction](#loading-models-for-prediction)\n",
    "8. [Prediction on New Data](#prediction-on-new-data)\n",
    "9. [Challenges and Solutions](#challenges-and-solutions)\n",
    "10. [Conclusion](#conclusion)\n",
    "12. [References](#references)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07432c02",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries <a name=\"importing-libraries\"></a>\n",
    "\n",
    "*We begin by importing all the necessary libraries required for data manipulation, visualization, preprocessing, and model building.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06995994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from pickle import dump\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cdb6e",
   "metadata": {},
   "source": [
    "## 2. Loading and Exploring the Data <a name=\"loading-and-exploring-the-data\"></a>\n",
    "\n",
    "### Dataset Description <a name=\"dataset-description\"></a>\n",
    "\n",
    "*The WELFake dataset is a comprehensive collection of news articles, merged from four popular datasets (Kaggle, McIntire, Reuters, BuzzFeed Political) to prevent overfitting and provide ample text data for machine learning training.*\n",
    "\n",
    "- **Total Entries:** 72,134 news articles\n",
    "  - **Real News:** 35,028 articles (Label = 1)\n",
    "  - **Fake News:** 37,106 articles (Label = 0)\n",
    "- **Columns:**\n",
    "  - `Serial number`: Unique identifier for each article\n",
    "  - `Title`: Headline of the news article\n",
    "  - `Text`: Main content of the news article\n",
    "  - `Label`: Indicates whether the news is real (1) or fake (0)\n",
    "\n",
    "### Initial Data Inspection <a name=\"initial-data-inspection\"></a>\n",
    "\n",
    "*We load the dataset and perform initial inspections to understand its structure and identify any immediate issues.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Data\n",
    "data = pd.read_csv(\"WELFake_Dataset.csv\")\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1974d8",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing <a name=\"data-preprocessing\"></a>\n",
    "\n",
    "### Handling Missing Values <a name=\"handling-missing-values\"></a>\n",
    "\n",
    "*We check for missing values and handle them appropriately to ensure data integrity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b44ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns='Unnamed: 0',inplace=True)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "print(data.isnull().sum())\n",
    "\n",
    "data.fillna(' ',inplace=True)\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14359d05",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis <a name=\"exploratory-data-analysis\"></a>\n",
    "\n",
    "#### Text Length Analysis <a name=\"text-length-analysis\"></a>\n",
    "\n",
    "*We analyze the distribution of text lengths to understand differences between fake and real news articles.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body_len'] = data['text'].apply(len)\n",
    "\n",
    "bins = np.linspace(0, 200, 40)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data[data[\"label\"]== 1][\"body_len\"], bins, alpha=0.5, label=\"Fake\", color=\"#FF5733\")\n",
    "plt.hist(data[data[\"label\"]== 0][\"body_len\"], bins, alpha=0.5, label=\"Real\", color=\"#33FFB8\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Body Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Text Length')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e688dc",
   "metadata": {},
   "source": [
    "### Text Cleaning <a name=\"text-cleaning\"></a>\n",
    "\n",
    "*We define a function to clean the text data, which includes several preprocessing steps to prepare the data for vectorization.*\n",
    "\n",
    "#### Tokenization, Stopword Removal, and Stemming <a name=\"text-cleaning-steps\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb513e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def clean_text(txt):\n",
    "        txt = txt.lower()\n",
    "        txt = word_tokenize(txt)\n",
    "        txt = [t for t in txt if t not in punctuation]\n",
    "        txt = [t for t in txt if t not in stopwords.words(\"english\")]\n",
    "        txt = [ps.stem(t)for t in txt]\n",
    "        txt = \" \".join(txt)\n",
    "        return txt\n",
    " \n",
    "\n",
    "data.loc[:,\"clean_text\"]=data[\"text\"].apply(clean_text)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d8021",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction <a name=\"feature-extraction\"></a>\n",
    "\n",
    "### TF-IDF Vectorization <a name=\"tf-idf-vectorization\"></a>\n",
    "\n",
    "*We transform the cleaned text data into numerical features using TF-IDF vectorization, which considers both term frequency and inverse document frequency.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d87fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer()\n",
    "vector = cv.fit_transform(c_data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ad56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame.sparse.from_spmatrix(vector,columns=cv.get_feature_names_out())\n",
    "target = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974395c",
   "metadata": {},
   "source": [
    "## 5. Model Training <a name=\"model-training\"></a>\n",
    "\n",
    "### Train-Test Split <a name=\"train-test-split\"></a>\n",
    "\n",
    "*We split the dataset into training and testing sets to evaluate the model's performance on unseen data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b278829",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d373727",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier <a name=\"multinomial-naive-bayes-classifier\"></a>\n",
    "\n",
    "*We train a Multinomial Naive Bayes classifier, which is suitable for text classification tasks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dbad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb=MultinomialNB()\n",
    "mnb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1009bc",
   "metadata": {},
   "source": [
    "### Random Forest Classifier <a name=\"random-forest-classifier\"></a>\n",
    "\n",
    "*We train a Random Forest classifier with 300 estimators to improve prediction accuracy.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36900cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=300)\n",
    "rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553cc188",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation <a name=\"model-evaluation\"></a>\n",
    "\n",
    "### Classification Reports <a name=\"classification-reports\"></a>\n",
    "\n",
    "*We evaluate both models using classification reports to compare their performance.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crnb = classification_report(y_test,mnb.predict(x_test))\n",
    "crf = classification_report(y_test,rf.predict(x_test))\n",
    "print(crnb,crf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff9843",
   "metadata": {},
   "source": [
    "## 7. Model Saving and Deployment <a name=\"model-saving-and-deployment\"></a>\n",
    "\n",
    "### Saving Models with Pickle <a name=\"saving-models-with-pickle\"></a>\n",
    "\n",
    "*We save the trained models and vectorizer using the pickle module for future use without retraining.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df02803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector creation\n",
    "f= open(\"CV_FRN.pkl\",\"wb\")\n",
    "dump(cv,f)\n",
    "f.close()\n",
    "\n",
    "# MultinomialNB model creation\n",
    "f= open(\"MNB_FRN.pkl\",\"wb\")\n",
    "dump(mnb,f)\n",
    "f.close()\n",
    "\n",
    "# RandomForestClassifier model creation\n",
    "f= open(\"RF_FRN.pkl\",\"wb\")\n",
    "dump(rf,f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7fef8e",
   "metadata": {},
   "source": [
    "### Loading Models for Prediction <a name=\"loading-models-for-prediction\"></a>\n",
    "\n",
    "*We demonstrate how to load the saved models and vectorizer to make predictions on new data.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23350800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laod in vector file for prediction\n",
    "from pickle import load\n",
    "\n",
    "f=open(\"CV_FRN.pkl\",\"rb\")\n",
    "cv=load(f)\n",
    "f.close()\n",
    "\n",
    "f=open(\"MNB_FRN.pkl\",\"rb\")\n",
    "mnb=load(f)\n",
    "f.close()\n",
    "\n",
    "f=open(\"RF_FRN.pkl\",\"rb\")\n",
    "rf=load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02810fac",
   "metadata": {},
   "source": [
    "## 8. Prediction on New Data <a name=\"prediction-on-new-data\"></a>\n",
    "\n",
    "*We accept user input, preprocess it, and use both models to predict whether the news is fake or real.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ded9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction \n",
    "\n",
    "news = input(\"enter news text \") \n",
    "\n",
    "# Cleaned user input data\n",
    "cnews=clean_text(news)\n",
    "\n",
    "# vectorize cleaned data\n",
    "vnews=cv.transform([cnews])\n",
    "\n",
    "# predict using both Model\n",
    "#MultinomialNB model\n",
    "pred_mnb=mnb.predict(vnews)\n",
    "\n",
    "#RandomForestClassifier model\n",
    "pred_rf=rf.predict(vnews)\n",
    "\n",
    "print(pred_rf[0],pred_mnb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ee81d",
   "metadata": {},
   "source": [
    "## 9. Challenges and Solutions <a name=\"challenges-and-solutions\"></a>\n",
    "\n",
    "*During the project, we faced several challenges due to the large size of the dataset and high dimensionality of the feature space.*\n",
    "\n",
    "- **Memory Limitations:**\n",
    "  - **Issue:** Memory errors occurred when processing n-grams and bigrams with `CountVectorizer`, resulting in over 600,000 features.\n",
    "  - **Solution:** Switched to `TfidfVectorizer` and used a sparse matrix representation to handle the large feature set efficiently.\n",
    "\n",
    "- **Processing Time:**\n",
    "  - **Issue:** Data splitting and model training were time-consuming, with some steps taking several hours.\n",
    "  - **Solution:** Opted for more efficient algorithms and limited the use of resource-intensive processes.\n",
    "\n",
    "- **Data Preprocessing Decisions:**\n",
    "  - **Issue:** Including stopwords led to increased dimensionality and memory issues.\n",
    "  - **Solution:** Removed stopwords to reduce the feature space and avoid memory constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Conclusion <a name=\"conclusion\"></a>\n",
    "\n",
    "*By carefully preprocessing the data and selecting appropriate models, we successfully built classifiers capable of distinguishing between fake and real news with high accuracy. The Random Forest Classifier performed better, achieving approximately 94% accuracy compared to 91% with Multinomial Naive Bayes. Despite challenges related to memory and processing time, the project demonstrates the effectiveness of machine learning in text classification tasks.*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 12. References <a name=\"references\"></a>\n",
    "\n",
    "- WELFake Dataset Publication: [IEEE Transactions on Computational Social Systems](https://doi.org/10.1109/TCSS.2021.3068519)\n",
    "- NLTK Documentation: [NLTK 3.6.2](https://www.nltk.org/)\n",
    "- Scikit-learn Documentation: [scikit-learn](https://scikit-learn.org/stable/)\n",
    "- Python Pickle Module: [pickle — Python object serialization](https://docs.python.org/3/library/pickle.html)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db62823",
   "metadata": {},
   "outputs": [],
   "source": [
    "python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e405846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
